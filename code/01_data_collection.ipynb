{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f97c6f56-0efa-4d03-a1ce-7147fc2ec16b",
   "metadata": {},
   "source": [
    "### **Data collection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7bf70a-56b9-4a95-86cf-3179c5a978b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "pd.options.display.max_colwidth = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6799d0d4-77f0-43a0-a9c6-adb9400126a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will define dictionaries with time stamps, with a gap of several days.\n",
    "# I will collect 100 submissions from each time stamp.\n",
    "# I took a several days gap so that we don't risk collecting the same submissions - in any case, we'll \n",
    "# check this again later on to make sure each time stamp in our data is unique.\n",
    "# For each date, the information pulled is from midnight (12:00:00 AM, GMT).\n",
    "# An Epoch Converter was used to get the UTC from the date (https://www.epochconverter.com/)\n",
    "\n",
    "# For some reason, when I tried 1 dictionary containing all the time stamps and 1 loop \n",
    "# that pulled down all the data together, it crashed each time after a different number of iterations. So\n",
    "# I had to split it into several pieces and run it several times separately for each subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec7d126-1aae-4504-9a55-1dae6aca1c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionaries with time stamps\n",
    "time_dict1 = {\n",
    "    1577836800 : '01/01/2020',\n",
    "    1578182400 : '01/05/2020',\n",
    "    1578614400 : '01/10/2020',\n",
    "    1579046400 : '01/15/2020',\n",
    "    1579478400 : '01/20/2020',\n",
    "    1579910400 : '01/25/2020',\n",
    "    1580428800 : '01/31/2020',\n",
    "    1580860800 : '02/05/2020',\n",
    "    1581292800 : '02/10/2020',\n",
    "    1581724800 : '02/15/2020',\n",
    "    1582156800 : '02/20/2020',\n",
    "    1582588800 : '02/25/2020' }\n",
    "\n",
    "time_dict2 = {\n",
    "    1582934400 : '02/29/2020',\n",
    "    1583366400 : '03/05/2020',\n",
    "    1583798400 : '03/10/2020',\n",
    "    1584230400 : '03/15/2020',\n",
    "    1584662400 : '03/20/2020',\n",
    "    1585094400 : '03/25/2020',\n",
    "    1585612800 : '03/31/2020',\n",
    "    1586044800 : '04/05/2020',\n",
    "    1586476800 : '04/10/2020',\n",
    "    1586908800 : '04/15/2020' }\n",
    "\n",
    "time_dict3 = {\n",
    "    1587340800 : '04/20/2020',\n",
    "    1587772800 : '04/25/2020',\n",
    "    1588204800 : '04/30/2020',\n",
    "    1588636800 : '05/05/2020',\n",
    "    1589068800 : '05/10/2020',\n",
    "    1589500800 : '05/15/2020',\n",
    "    1589932800 : '05/20/2020',\n",
    "    1590364800 : '05/25/2020',\n",
    "    1590883200 : '05/31/2020',\n",
    "    1591315200 : '06/05/2020' }\n",
    "\n",
    "time_dict4 = {\n",
    "    1591747200 : '06/10/2020',\n",
    "    1592179200 : '06/15/2020',\n",
    "    1592611200 : '06/20/2020',\n",
    "    1593043200 : '06/25/2020',\n",
    "    1593475200 : '06/30/2020',\n",
    "    1593907200 : '07/05/2020',\n",
    "    1594339200 : '07/10/2020',\n",
    "    1594771200 : '07/15/2020',\n",
    "    1595203200 : '07/20/2020',\n",
    "    1595635200 : '07/25/2020' }\n",
    "\n",
    "time_dict5 = {\n",
    "    1596153600 : '07/31/2020',\n",
    "    1596585600 : '08/05/2020',\n",
    "    1597017600 : '08/10/2020',\n",
    "    1597449600 : '08/15/2020',\n",
    "    1597881600 : '08/20/2020',\n",
    "    1598313600 : '08/25/2020',\n",
    "    1598832000 : '08/31/2020',\n",
    "    1599264000 : '09/05/2020',\n",
    "    1599696000 : '09/10/2020',\n",
    "    1600128000 : '09/15/2020' }\n",
    "\n",
    "time_dict6 = {\n",
    "    1600560000 : '09/20/2020',\n",
    "    1600992000 : '09/25/2020',\n",
    "    1601424000 : '09/30/2020',\n",
    "    1601856000 : '10/05/2020',\n",
    "    1602288000 : '10/10/2020',\n",
    "    1602720000 : '10/15/2020',\n",
    "    1603152000 : '10/20/2020',\n",
    "    1603584000 : '10/25/2020',\n",
    "    1604016000 : '10/30/2020',\n",
    "    1604534400 : '11/05/2020' }\n",
    "\n",
    "time_dict7 = {\n",
    "    1604966400 : '11/10/2020',\n",
    "    1605398400 : '11/15/2020',\n",
    "    1605830400 : '11/20/2020',\n",
    "    1606262400 : '11/25/2020',\n",
    "    1606694400 : '11/30/2020',\n",
    "    1607126400 : '12/05/2020',\n",
    "    1607558400 : '12/10/2020',\n",
    "    1607990400 : '12/15/2020',\n",
    "    1608422400 : '12/20/2020',\n",
    "    1608854400 : '12/25/2020' }\n",
    "\n",
    "time_dict8 = {\n",
    "    1609286400 : '12/30/2020',\n",
    "    1609804800 : '01/05/2021',\n",
    "    1610236800 : '01/10/2021',\n",
    "    1610668800 : '01/15/2021',\n",
    "    1611100800 : '01/20/2021',\n",
    "    1611532800 : '01/25/2021',\n",
    "    1611964800 : '01/30/2021',\n",
    "    1612483200 : '02/05/2021',\n",
    "    1612915200 : '02/10/2021',\n",
    "    1613347200 : '02/15/2021' }\n",
    "\n",
    "time_dict9 = {\n",
    "    1613779200 : '02/20/2021',\n",
    "    1614470400 : '02/28/2021',\n",
    "    1614556800 : '03/01/2021',\n",
    "    1614902400 : '03/05/2021',\n",
    "    1615334400 : '03/10/2021',\n",
    "    1615766400 : '03/15/2021',\n",
    "    1616198400 : '03/20/2021',\n",
    "    1616630400 : '03/25/2021',\n",
    "    1617062400 : '03/30/2021',\n",
    "    1617580800 : '04/05/2021' }\n",
    "\n",
    "time_dict10 = {\n",
    "    1618012800 : '04/10/2021',\n",
    "    1618444800 : '04/15/2021',\n",
    "    1618876800 : '04/20/2021',\n",
    "    1619308800 : '04/25/2021',\n",
    "    1619740800 : '04/30/2021',\n",
    "    1620172800 : '05/05/2021',\n",
    "    1620604800 : '05/10/2021',\n",
    "    1621036800 : '05/15/2021',\n",
    "    1621468800 : '05/20/2021',\n",
    "    1621900800 : '05/25/2021',\n",
    "    1622332800 : '05/30/2021' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf7a9f41-080f-4fe1-b119-46afc7ac4c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 93 with time stamp: 1618012800...\n",
      "... done.\n",
      "Submission 94 with time stamp: 1618444800...\n",
      "... done.\n",
      "Submission 95 with time stamp: 1618876800...\n",
      "... done.\n",
      "Submission 96 with time stamp: 1619308800...\n",
      "... done.\n",
      "Submission 97 with time stamp: 1619740800...\n",
      "... done.\n",
      "Submission 98 with time stamp: 1620172800...\n",
      "... done.\n",
      "Submission 99 with time stamp: 1620604800...\n",
      "... done.\n",
      "Submission 100 with time stamp: 1621036800...\n",
      "... done.\n",
      "Submission 101 with time stamp: 1621468800...\n",
      "... done.\n",
      "Submission 102 with time stamp: 1621900800...\n",
      "... done.\n",
      "Submission 103 with time stamp: 1622332800...\n",
      "... done.\n"
     ]
    }
   ],
   "source": [
    "## collecting data for 'datascience' subreddit ##\n",
    "\n",
    "# URL\n",
    "url = 'https://api.pushshift.io//reddit/search/submission'\n",
    "\n",
    "# The for loop below will request the submissions.\n",
    "# The time period is determined by the time dictionaries defined in the previous cell.\n",
    " \n",
    "# The loop should be run separately for each time dictionary (10 times) for the subreddit 'datascience'; \n",
    "# in the next cell the same is done for the subreddit 'books' (using the same 10 time dictionaries).\n",
    "\n",
    "# Define a count variable to count the number of data pulling operations we make - we'll use it for the \n",
    "# file name. For each time we run the loop, we need to start the count variable from a different value, \n",
    "# so that each file name will be unique.\n",
    "\n",
    "count = 1 # for time_dict1 \n",
    "# count = 13 # for time_dict2\n",
    "# count = 23 # for time_dict3\n",
    "# count = 33 # for time_dict4\n",
    "# count = 43 # for time_dict5\n",
    "# count = 53 # for time_dict6\n",
    "# count = 63 # for time_dict7\n",
    "# count = 73 # for time_dict8\n",
    "# count = 83 # for time_dict9\n",
    "# count = 93 # for time_dict10\n",
    "\n",
    "# for loop to request the data\n",
    "for time in list(time_dict1.keys()): # change here once for time_dict1, once for time_dict2, etc. until \n",
    "                                     # time_dict10\n",
    "    \n",
    "    # print loop progress\n",
    "    print(f'Submission {count} with time stamp: {time}...')\n",
    "    \n",
    "    # set parameters to specify which submissions to take\n",
    "    params = {\n",
    "        'subreddit' : 'datascience',   # subreddit     \n",
    "        'size' :  100,                 # how many submissions (max allowed 100 at once)\n",
    "        'before' :  time               # date from which to get the subreddits\n",
    "    }\n",
    "    \n",
    "    # requesting the posts\n",
    "    response = requests.get(url, params)\n",
    "    \n",
    "    # transforming the information into a data frame\n",
    "    df = pd.DataFrame(response.json()['data'])\n",
    "    \n",
    "    # extracting the relevant columns\n",
    "    df = df[['subreddit', 'selftext', 'title', 'created_utc']]\n",
    "        \n",
    "    # save df as a csv file\n",
    "    df.to_csv('../datafiles/datascience' + str(count) +'.csv', index=False)\n",
    "    \n",
    "    # update the count variable\n",
    "    count += 1\n",
    "        \n",
    "    # print info on loop progress\n",
    "    print('... done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "335b112b-9e83-4303-a9a4-3a07b2949ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission 93 with time stamp: 1618012800...\n",
      "... done.\n",
      "Submission 94 with time stamp: 1618444800...\n",
      "... done.\n",
      "Submission 95 with time stamp: 1618876800...\n",
      "... done.\n",
      "Submission 96 with time stamp: 1619308800...\n",
      "... done.\n",
      "Submission 97 with time stamp: 1619740800...\n",
      "... done.\n",
      "Submission 98 with time stamp: 1620172800...\n",
      "... done.\n",
      "Submission 99 with time stamp: 1620604800...\n",
      "... done.\n",
      "Submission 100 with time stamp: 1621036800...\n",
      "... done.\n",
      "Submission 101 with time stamp: 1621468800...\n",
      "... done.\n",
      "Submission 102 with time stamp: 1621900800...\n",
      "... done.\n",
      "Submission 103 with time stamp: 1622332800...\n",
      "... done.\n"
     ]
    }
   ],
   "source": [
    "## Same operation as above for the 'books' subreddit ##\n",
    "\n",
    "# URL\n",
    "url = 'https://api.pushshift.io//reddit/search/submission'\n",
    "\n",
    "# count variable\n",
    "\n",
    "count = 1 # for time_dict1 \n",
    "# count = 13 # for time_dict2\n",
    "# count = 23 # for time_dict3\n",
    "# count = 33 # for time_dict4\n",
    "# count = 43 # for time_dict5\n",
    "# count = 53 # for time_dict6\n",
    "# count = 63 # for time_dict7\n",
    "# count = 73 # for time_dict8\n",
    "# count = 83 # for time_dict9\n",
    "# count = 93 # for time_dict10\n",
    "\n",
    "# for loop to collect the data\n",
    "for time in list(time_dict1.keys()):\n",
    "    \n",
    "    # print loop progress\n",
    "    print(f'Submission {count} with time stamp: {time}...')\n",
    "    \n",
    "    # set parameters to specify which submissions to pull in\n",
    "    params = {\n",
    "        'subreddit' : 'books',        # subreddit     \n",
    "        'size' :  100,                 # how many submissions (max allowed 100 at once)\n",
    "        'before' :  time               # date from which to get the subreddits\n",
    "    }\n",
    "    \n",
    "    # pulling down the submissions\n",
    "    response = requests.get(url, params)\n",
    "    \n",
    "    # transforming the information into a data frame\n",
    "    df = pd.DataFrame(response.json()['data'])\n",
    "    \n",
    "    # extracting the columns for the NLP model\n",
    "    df = df[['subreddit', 'selftext', 'title', 'created_utc']]\n",
    "        \n",
    "    # save df as a csv file\n",
    "    df.to_csv('../datafiles/books' + str(count) +'.csv', index=False)\n",
    "    \n",
    "    # update the count variable\n",
    "    count += 1\n",
    "        \n",
    "    # print info on loop progress\n",
    "    print('... done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e33da32-da0e-40ff-a686-0257b8f3deed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2a50206-b009-4587-8b31-5a009d6598ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make a list of the data files\n",
    "files = os.listdir('../datafiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05aca15e-6bf4-4846-940c-d4aeb2d93a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.ipynb_checkpoints'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the first file(s) - they might be hidden files that we don't need\n",
    "files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "325afc25-dc77-4a06-8c2a-72c5597dc041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we don't include the ipnyb file\n",
    "files = files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec351c50-04cf-4151-843d-eb51a23faef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## putting all the data files together ##\n",
    "\n",
    "# load the first file\n",
    "data = pd.read_csv('../datafiles/'+files[0])\n",
    "\n",
    "# for loop to concatenate all data frames together\n",
    "for file in range(1, len(files)):\n",
    "    # loading the next file in the line    \n",
    "    df = pd.read_csv('../datafiles/'+files[file])\n",
    "    # pasting the data frames together\n",
    "    data = pd.concat([data, df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed613b1d-5195-40c7-b099-65fda8830a2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check uniquness of UTC time stamps in the data frame\n",
    "# the resulting list should be empty\n",
    "dupes = [x for n, x in enumerate(data['created_utc']) if x in data['created_utc'][:n]] # code from https://stackoverflow.com/questions/9835762/how-do-i-find-the-duplicates-in-a-list-and-create-another-list-with-them\n",
    "dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55fc0111-e1e2-4103-a860-8bcc41e7c312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20600, 4)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the big data frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "410b1bec-df1f-4408-82a3-7b944f144e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>books</td>\n",
       "      <td>And it's not because I'm a slow reader - quite the contrary. But this was a book worth taking my time over.\\n\\nI think it's mostly to do with the fact that I knew beforehand how the story of this remarkable ship ends, and I was putting off reading about the details.\\n\\nCouple this with the wonderful and charming way in which Palin (of Monty Pyt...</td>\n",
       "      <td>It's taken me all year, but I finally finished \"Erebus: The Story of a Ship\" by Michael Palin</td>\n",
       "      <td>1577836753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>books</td>\n",
       "      <td>Those that scroll through reddit AND read so many books, some of you read sooooo many, how do you find the time?</td>\n",
       "      <td>How do you find the time?</td>\n",
       "      <td>1577835261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>What’s everyone’s reading goals for next year? I’d like to read 50 books and read more fiction.</td>\n",
       "      <td>2020 reading goals</td>\n",
       "      <td>1577834790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books</td>\n",
       "      <td>Has anyone read Cage of Souls and what did you think of it? Never read anything by Tchaikovsky before but if I like cage or Souls I'll try 'Children of Time' series. I haven't read cage of Souls yet so no spoilers please</td>\n",
       "      <td>Cage of Souls - Adrian Tchaikovsky</td>\n",
       "      <td>1577834642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>books</td>\n",
       "      <td>For the past few years, one of my New Year's resolutions was to read 100 books I had not previously read before. I had the same resolution for 2019, but this time I bothered to keep track of the books I had read (and finished). In the end, I managed to finish 58. However, I am happy with the result, since I can say I kept track of what I was re...</td>\n",
       "      <td>Tracking the Books I Read In 2019</td>\n",
       "      <td>1577834517</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit  \\\n",
       "0     books   \n",
       "1     books   \n",
       "2     books   \n",
       "3     books   \n",
       "4     books   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                        selftext  \\\n",
       "0  And it's not because I'm a slow reader - quite the contrary. But this was a book worth taking my time over.\\n\\nI think it's mostly to do with the fact that I knew beforehand how the story of this remarkable ship ends, and I was putting off reading about the details.\\n\\nCouple this with the wonderful and charming way in which Palin (of Monty Pyt...   \n",
       "1                                                                                                                                                                                                                                               Those that scroll through reddit AND read so many books, some of you read sooooo many, how do you find the time?   \n",
       "2                                                                                                                                                                                                                                                                What’s everyone’s reading goals for next year? I’d like to read 50 books and read more fiction.   \n",
       "3                                                                                                                                   Has anyone read Cage of Souls and what did you think of it? Never read anything by Tchaikovsky before but if I like cage or Souls I'll try 'Children of Time' series. I haven't read cage of Souls yet so no spoilers please   \n",
       "4  For the past few years, one of my New Year's resolutions was to read 100 books I had not previously read before. I had the same resolution for 2019, but this time I bothered to keep track of the books I had read (and finished). In the end, I managed to finish 58. However, I am happy with the result, since I can say I kept track of what I was re...   \n",
       "\n",
       "                                                                                           title  \\\n",
       "0  It's taken me all year, but I finally finished \"Erebus: The Story of a Ship\" by Michael Palin   \n",
       "1                                                                      How do you find the time?   \n",
       "2                                                                             2020 reading goals   \n",
       "3                                                             Cage of Souls - Adrian Tchaikovsky   \n",
       "4                                                              Tracking the Books I Read In 2019   \n",
       "\n",
       "   created_utc  \n",
       "0   1577836753  \n",
       "1   1577835261  \n",
       "2   1577834790  \n",
       "3   1577834642  \n",
       "4   1577834517  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print first 5 rows\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5a20c3f-ccc7-40a7-9274-8f7c52ce70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data frame\n",
    "data.to_csv('../data/data_reddit.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2183d724-2c47-421c-b324-a9bb04de4332",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
